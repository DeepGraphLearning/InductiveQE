output_dir: .~/git/InductiveQE/experiments

dataset:
  class: InductiveFB15k237CompExtendedEval  # special task to evaluate train queries on the bigger test graph
  path: ~/git/InductiveQE/data
  ratio: {{ ratio }}                # specify dataset ratio here or when running the script

task:
  class: InductiveLogicalQuery
  model:
    class: GNN-QE
    model:
      class: NBFNet
      input_dim: 32                 # Hyperparams from Table 14 (Appendix D of the paper)
      hidden_dims: [32, 32, 32, 32]
      message_func: distmult
      aggregate_func: pna
      short_cut: yes
      layer_norm: yes
      dependent: yes
    logic: product
    dropout_ratio: 0.5
  criterion: bce
  sample_weight: no
  adversarial_temperature: 0.1

optimizer:
  class: Adam
  lr: 5.0e-3

engine:
  gpus: {{ gpus }}
  batch_size: 64                    # reduce if doesn't fit on a GPU

train:
  num_epoch: 0                      # no training, just run the inference
  batch_per_epoch: 1000             # number of batches to be considered as "one epoch"

checkpoint: {{ checkpoint }}        # path to the checkpoint of the trained model
metric: mrr
skip_eval_on_train: False           # here, we do run evaluation of train queries on training and test graphs
save_embs: False                    # doesn't work for GNN-QE as it saves its checkpoints automatically