output_dir: ~/git/InductiveQE/experiments

dataset:
  class: InductiveFB15k237DatasetLP
  path: ~/git/InductiveQE/data
  ratio: 550

task:
  class: InductiveKnowledgeGraphCompletion
  model:
    class: NodePiece
    input_dim: 1000                           # Hyperparams from Table 10 (Appendix D of the paper)
    rel_context: 10
    num_anchors: 0                            # turn off anchor tokenizing
    ancs_per_node: 10                         # not used
    hidden_dims: [200, 200, 200, 200, 200]    # not used for no-GNN model
    scoring_function: complex
    pooler: 2sum                              # random projection pooler
    message_func: rotate
    aggregate_func: sum
    short_cut: yes
    layer_norm: yes
    gnn: no                                   # no GNN for this model
    use_boundary: False
    use_inverses: True
  criterion: bce
  num_negative: 128
  strict_negative: yes
  adversarial_temperature: {{ temp }}         # adversarial temperature might vary for different datasets
  sample_weight: no

optimizer:
  class: Adam
  lr: 1.0e-4

engine:
  gpus: {{ gpus }}
  batch_size: 512
  logger: {{ logger }}                        # 'wandb' or 'console' (w/o quotes)

train:
  num_epoch: {{ epochs }}                     # desired number of epochs to train

metric: mrr
save_embs: True                               # save entity/relation embeddings of the test graph after training
skip_eval_on_train: True                      # do not perform evaluation on the training set
